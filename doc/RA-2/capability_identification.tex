\section{Step RA-2: Capability Identification}
In this step the flows of different views of the system are modeled using BPMN diagrams. Then capabilities are identified by mapping these functional requirements with RefSORS requirements and analyzing the each flow. Finally these capabilities are assessed based on their functionalities to decide which are going to be exposed as services and which are going to be provided as components that support these services.

\subsection{System Flow Model}
From functional requirements and concepts from the reference architecture BPMN diagrams were modeled to represent the application flow. The modeled application flow was divided in six BPMN diagrams and seven polls shown in order of occurrence, each lane represents a possible capability and tasks are inspired by the robotic system required functionalities.
Figure 1 shows the most abstract flow, the flow of the broadest system. Each robot initializes and requests the map to the master, that delivers it. Then they determine their own positions. After they know their own positions, the master can process the product transport request. It asks the distance of the robots from the product, chooses the closest one, and continues to wait for other requests.

%Broad System Flow
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.23]{./RA-2/broadSystemFlow.png}
 \caption{Figure 1: Broad System Flow}
 \label{fig:broadsystemflow}
\end{figure}

Figure 1: This BPMN diagram shows how the individual robots decide wether to answer to a transport request or not.

The robot receives a transport request. After the request is received, it gets the product's position and it's own position in the map then calculates the Manhattan distance between them and broadcasts it. The distance calculated by it is compared to the distances calculated by other robots, if the former is smaller it broadcasts a message saying that it will take the request. If there is any robot closer to the product it waits for another request.



%Move to Destination Flow
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.282]{./RA-2/moveToDestination.png}
 \caption{Figure 2: Move to Destination Robot Flow}
 \label{fig:movetodestination}
\end{figure}
Figure 2: This BPMN diagram shows how the robot moves to a destination.

Upon a need to navigate to a destination on the map, the robot needs it's own position and the other robots positions. When this positions needed are aquired, it calculates the best path using a dijkstra algorithm but removing the pathways blocked by other robots. Then while a path recalculation triggering event is waited the last calculated path is executed. The execution of the path is done by following the guide line until a crossing is found, then it chooses wether to go straigth, turn left or turn right based on the calculated path. Everytime the robot goes by a crossing it broadcasts it's location.

%Pick Up Product Flow
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.276]{./RA-2/pickup.png}
 \caption{Figure 3: Pick Up Product Flow}
 \label{fig:pickup}
\end{figure}

Figure 3: This BPMN diagram shows how the robot picks up a product.

When the robot reaches the product's position, meaning the product is close by, the fork is moved to predetermined height and the sonars are used to find where the product is. Then the robot moves to position the fork under the product. It repeates the sensing and moving process until the fork is in a satisfying place when it lifts the product.

%Release Product
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.33]{./RA-2/release.png}
 \caption{Figure 4: Release Product Flow}
 \label{fig:release}
\end{figure}

Figure 4: This BPMN diagram shows the process of releasing the product on the ground.

Once the product reaches destination the robot stops and starts the product drop off. The fork is lower until product reaches the ground and the robot moves straight backward until the fork is far away enough from the product so it can move it to navigation position.

%Subscribe to Sensors Flow
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.45]{./RA-2/subscribeSensors.png}
 \caption{Figure 4: Subscribe to Sensors Flow}
 \label{fig:subscribeSensors}
\end{figure}

Figure 5: This BPMN diagram shows the generic flow of subscribing to sensors and handling their data.

This flow happens during the robot's initialization. After subscribing to sonars topic the distances are get and handled, simultaneously the distances are updated. The same thing happens to the camera, but instead of distances images are handled.

%Follow Guide Line Flow
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.359]{./RA-2/followLine.png}
 \caption{Figure 1: Follow Guide Line Flow}
 \label{fig:followline}
\end{figure}

Figure 6: This BPMN diagram shows how the robot follows the guide line.

To follow the guide line the robot uses the camera, it gets the image and processes it. If the line is facing right or is translated to the right it turns right, if the line is facing forward and is at the center it goes straingth else if the line is facing left or translated to the left it turns left. If the line is facing one orientation and translated to the other it takes the decision based on the translation.

\subsection{Decompose Robot Application}

The BPMN diagrams are used to indentify capabilities. The process of analysing the BPMN diagrams to identify capabilities entails in comparing the BPMN to the functional requirements and RefSORS requirements. Due to how the system was modeled using the diagrams, most BPMN lanes are potential capabilities. Then the potential capabilities are compared to functional requirements and RefSORS requirements, if they fulfill at least part of a functional or RefSORS requirement they go to the next step. They next step is compare them to the domain concepts and RefSORS capabilities, if they are related they become capabilities.

%Robotic Application: 1..9 | 1 2 3 4 5 6 7 8 9
%Robotic Agent: 6..9 | 6 7 8 9
%Task: 10..19 | 10 11 12 13 14 15 16 17 18 19
%Knowledge: 20..23 | 20 21 22 23
%Device Driver: 24..30 | 24 25 26 27 28 29 30
%General: 31 32
{
\centering
\begin{tabular}{| r | p{2cm} | c | c |}
	\hline
	Functional Requirement & RefSORS Requirement & Domain Concept & Capability \\ 
	\hline
	FR1, FR2, FR6 & R-R1 	& Application & Application \\
							%& R-R2 	& Application & \\
	FR3, FR4, FR7, FR31 	& R-R3 & Application & Application\\
	FR23 					& R-R3 & Application & Mapping\\ %??
	FR22 					& R-R3 & Application & Control\\ %??
							%& R-R4 	& Application & \\
	FR6, FR7 				& R-R5 & Application & Application\\
							%& R-R6 	& Robotic Agent & \\
							%& R-R7 	& Robotic Agent & \\
	\hline
	FR28 					& R-R8 & Robotic Agent & Robot Agent\\
							%& R-R9 	& Robotic Agent & \\
							%& R-R10 & Task & \\
	\hline
	FR12 					& R-R11 & Task & Mapping\\
	FR22 					& R-R11 & Task & Control\\
	FR26				 	& R-R12 & Task & Control\\
	FR28				 	& R-R12 & Task & Robot Agent\\%Robot agent?
	FR29, FR30			 	& R-R12 & Task & Application\\%Application? (Operation no edusig)
	FR19, FR20, FR27		& R-R13 & Task & Object Manipulation\\
	FR12, FR16				& R-R14 & Task & Mapping\\
	FR17, FR18 				& R-R14 & Task & Localization\\
	FR12 					& R-R15 & Task & Mapping\\
	R15, FR21 				& R-R15 & Task & Navigation\\
	FR23, FR35				& R-R15 & Task & Path Planning\\
	FR12, FR13 				& R-R16 & Task & Mapping\\
	FR24, FR23				& R-R16 & Task & Path Planning\\
	FR12, FR13 				& R-R17 & Task & Mapping\\
							%& R-R18 & Task & \\
	FR10 					& R-R19 & Task & Camera Sensor\\
	FR14	 				& R-R19 & Task & Image Processing\\
	\hline
	FR11 					& R-R20 & Knowledge & Sonar Sensor\\
	FR13 					& R-R20 & Knowledge & Mapping\\ %Map information?
	FR22 					& R-R21 & Knowledge & Control\\
	FR23 					& R-R21 & Knowledge & Mapping\\
							%& R-R22 & Knowledge & \\
							%& R-R23 & Knowledge & \\
	\hline
	FR10 					& R-R24 & Device Driver & Camera Sensor\\
	FR11 					& R-R24 & Device Driver & Sonar Sensor\\
							%& R-R25 & Device Driver & \\
	FR22 					& R-R26 & Device Driver & Control\\
							%& R-R27 & Device Driver & \\
							%& R-R28 & Device Driver & \\
	FR8 					& R-R29 & Device Driver & Drive Actuator\\
	FR9 					& R-R29 & Device Driver & For Lift Actuator\\
	\hline
\end{tabular}
}


%Capabilities
\begin{figure}[ht!]
 \centering
 \includegraphics[scale=.3]{./RA-2/capabilityDiagram.png}
 \caption{Figure 9: Capabilites}
 \label{fig:capabilities}
\end{figure}

Figure 9: The diagram shows the identified capabilities, with their functionalities and relationships with each other.

\subsection{Rationalize Capabilities}
After identifying the capabilities, they must be analyzed to decide whether they will become services. Capabilities might also join other capabilities to form a service if they are too simple. They might also form more than one service if they are too complex. Table 2.3 shows the analysis of each capability and if they became one or more services or not at all.

{
\centering
\begin{tabular}{| l | p{6.6cm} | c |}
	\hline
	Capability & Analysis & Service\\
	\hline
	Camera Sensor & Needed to communicate with camera sensor. & yes\\
	Sonar Sensor & Needed to communicate with sonar sensor. & yes\\
	Fork Lift Actuator & Needed to communicate with fork lift actuator. & yes\\
	Drive Actuator & Needed to communicate with drive actuator. & yes\\
	\hline
	Image Processing & Needed to process images to identify the guide lines. & yes\\
	Mapping & Grid mapping to navigate and know other robots' positions. &\\
	Localization & Know it's own position on the grid. &\\ %(maybe too simple, join with mapping?, analyze scalability. Possible to create more complex localization?)
	Navigation & Navigate the grid, although simple, needs to be concise in case of change of logic and maintenance. & yes\\
	Path Planning & Calculate path to destination. & yes\\
	Object Manipulation & Logic of picking up and releasing product. & yes\\
	Control & Control unit. & yes\\
	Robotic Agent & Abstration of the robots functionalities. & yes\\
	Application & Interact with operator, send transport messages. & yes\\
	\hline
\end{tabular}
}

